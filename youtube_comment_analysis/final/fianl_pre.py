#!/usr/bin/env python
# coding: utf-8

# In[1]:


# -*- coding: utf-8 -*-
"""유튜브_댓글_분석_안성근.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DhPGwp5xjkm3_QaQfZW_o1irN8XaSdbZ

# 텍스트 분석 라이브러리 초기화
"""
# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
# 시각화 결과가 선명하게 표시
# %config InlineBackend.figure_fromat = 'retina'
# range 대신 처리 시간을 알려주는 라이브러리
from tqdm import trange
pd.options.display.float_format = '{:.2f}'.format


# In[2]:


"""# 시각화를 위한 한글폰트 설정"""

# 윈도우 한글폰트 설정
plt.rc("font", family='Malgun Gothic')


# In[3]:


# 유튜브 크롤링 파일 로드
path = '/home/lab10/final/raw/'

frames=[]
comment_file = 'stats_page_640세븐틴.csv'  #세븐틴
df = pd.read_csv(path+comment_file, encoding='utf-8', header=None)
print(df)
print('\n')


# In[4]:


frames.append(df)
df = pd.concat(frames, ignore_index=True)
df.columns=['comment','like']


# In[5]:


df


# In[6]:


df.describe()


# In[7]:


df.info()


# In[8]:


df.shape


# In[9]:


df.count()


# In[10]:


df.value_counts()


# In[11]:


import re


# In[12]:


df.like[0]


# In[43]:


like_num=[]
for i in range(len(df.like)):
    if 'K' in df.like[i]:
        tokens=re.sub('[K]','',df.like[i])
        tokens=float(tokens)
        tokens=tokens*1000
        like_num.append(tokens)
    else:
        like_num.append(df.like[i])
df['like_num']=like_num


# In[44]:


df


# In[45]:


df.describe()


# In[46]:


df.info()


# In[50]:


df['like_num']=pd.to_numeric(df['like_num'])


# In[52]:


df['like_num'].describe(percentiles=[0.75,0.8,0.85,0.9,0.95,0.99,0.999,0.9999])


# In[53]:


df['like_num'].plot()


# In[54]:


df.fillna('None')


# In[56]:


df.to_csv('/home/lab10/final/raw/num/stats_page_640세븐틴.csv')


# In[57]:


"""# 언어별 분류 작업
- 정확도가 높은 fasttext 모듈로 분류
"""
get_ipython().system('pip install fasttext')
import copy
import fasttext
import pandas as pd
import re


# In[58]:


df


# In[60]:


# 중복 값 제거
print("전처리 시작")
print('\n')
print('중복 제거 전 :',df.shape)
df = df.drop_duplicates(['comment'],keep='last',ignore_index=True)
print('중복 제거 후 :',df.shape)
print('\n')

# 소문자로 바꾸기
df['comment'] = df['comment'].str.lower()


# In[61]:


# 전처리 전 원본 보존
copy_data = copy.deepcopy(df)


# In[62]:


copy_data.info()


# In[64]:


copy_data


# In[65]:


emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                        "]+", flags=re.UNICODE)

#분석에 어긋나는 불용어구 제외 (특수문자, 의성어)
han = re.compile(r'[ㄱ-ㅎㅏ-ㅣ!?~,".\n\r#\ufeff\u200d]')

comment_result = []

for i in copy_data['comment'].values:
    tokens = re.sub(emoji_pattern,"",i)
    tokens = re.sub(han,"",tokens)
    comment_result.append(tokens)

punct = "/-'?!.,#$%\'()*+-/:;<=>@[\\]^_`{|}~" + '""“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\×™√²—–&'

punct_mapping = {"‘": "'", "₹": "e", "´": "'", "°": "", "€": "e", "™": "tm", "√": " sqrt ", "×": "x", "²": "2", "—": "-", "–": "-", "’": "'", "_": "-", "`": "'", '“': '"', '”': '"', '“': '"', "£": "e", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }


# In[66]:


def clean_punc(text, punct, mapping):
    for p in mapping:
        text = text.replace(p, mapping[p])

    for p in punct:
        text = text.replace(p, f' {p} ')

    specials = {'\u200b': ' ', '…': ' ... ', '\ufeff': '', 'करना': '', 'है': ''}
    for s in specials:
        text = text.replace(s, specials[s])

    return text.strip()

cleaned_corpus = []
for sent in comment_result:
    cleaned_corpus.append(clean_punc(sent, punct, punct_mapping))


# In[67]:


def clean_text(texts):
    corpus = []
    for i in range(0, len(texts)):
        review = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\"]', '',str(texts[i])) #remove punctuation
#         review = re.sub(r'\d+','', str(texts[i]))# remove number
        review = review.lower() #lower case
        review = re.sub(r'\s+', ' ', review) #remove extra space
        review = re.sub(r'<[^>]+>','',review) #remove Html tags
        review = re.sub(r'\s+', ' ', review) #remove spaces
        review = re.sub(r"^\s+", '', review) #remove space from start
        review = re.sub(r'\s+$', '', review) #remove space from the end
        corpus.append(review)
    return corpus

basic_preprocessed_corpus = clean_text(cleaned_corpus)
comment_result = pd.DataFrame(basic_preprocessed_corpus, columns=["comment"])


# In[68]:


comment_result


# In[69]:


model = fasttext.load_model('/home/lab10/official_youtube/lid.176.ftz')

predict = []
for t in comment_result.comment.values:
    predict.append(model.predict(t,k=1))

ty = pd.DataFrame(predict)


# In[70]:


comment = []
for num, txt in enumerate(ty[0]):
    txt = str(txt)
    if txt == "('__label__ko',)":
        b = re.sub(txt,"ko",txt)
        comment.append(b)
    elif txt == "('__label__en',)":
        b = re.sub(txt,"en",txt)
        comment.append(b)
    elif txt == "('__label__id',)":
        b = re.sub(txt,"id",txt)
        comment.append(b)
    elif txt == "('__label__es',)":
        b = re.sub(txt,"es",txt)
        comment.append(b)
    else:
        b = re.sub(txt,"etc",txt)
        comment.append(b)

comment = pd.DataFrame(comment)
print('\n')
print(f'GOT7댓글 언어 구성')
print(comment.value_counts())
print('\n')
pd.set_option('max_columns',50)
pd.set_option('max_rows',100)
# ty_sum.to_csv('ty_sum.csv', encoding='cp949')


# In[72]:


like = pd.DataFrame(copy_data['like_num'])
data = pd.concat([comment_result,like, comment],axis=1)
data.columns = ['comment','like','lang']


path_preprocess='/home/lab10/final/pre/'
data.to_csv(path_preprocess+'prepro_'+comment_file,  encoding='utf-8', header=None, index=None)

print(data[:])
print(f"전처리 끝")
print('\n')


# In[ ]:




