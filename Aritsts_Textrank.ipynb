{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aritsts_Textrank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YOCNZnracf7T3V9K-Kxy8ieU3meXpIA7",
      "authorship_tag": "ABX9TyP/GdNmU7+z1x8XRpa7RNQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IDF13/mulcam_army/blob/sumin/Aritsts_Textrank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "4Wo0oM_Xe88N",
        "outputId": "0e316088-7f1f-4b70-e26b-166b29579e6a"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"k-means_code.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Ls1XFeatzdMivoFijtX0MuN9E7ADviAC\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "\"\"\"# 데이터 불러오기\n",
        "댓글을 얼마나 불러와야 할까\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 아티스트 51팀의 데이터 불러옴 (전처리가 잘 되지 않은 7개 아티스트 제외)\n",
        "import os\n",
        "\n",
        "path = '/content/drive/MyDrive/[공유] Mulcam_Army 공유폴더!/크롤링 한 자료/k-pop_Radar아티스트 크롤링 할당분/수민_결과물/전처리 완료/'\n",
        "file_list = os.listdir(path)\n",
        "# file_list_py = [file for file in file_list if file.endswith('.csv')] ## 파일명 끝이 .csv인 경우\n",
        "# https://boleumdal.tistory.com/entry/python-%ED%8F%B4%EB%8D%94-%EC%95%88%EC%97%90-\\\n",
        "# %EC%9E%88%EB%8A%94-%ED%8C%8C%EC%9D%BC-%ED%95%9C%EB%B2%88%EC%97%90-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0\n",
        "\n",
        "\n",
        "# csv 파일들을 DataFrame으로 불러와서 concat\n",
        "df = pd.DataFrame()\n",
        "for i in file_list:\n",
        "    data = pd.read_csv(path + i, encoding='utf-8', header=None, engine='python')\n",
        "    df = pd.concat([df,data])\n",
        "    \n",
        "df = df.reset_index(drop=True)\n",
        "df.dropna(axis=0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>even if you re not a carat let s all admit it ...</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this song tells that some song s don t need bi...</td>\n",
              "      <td>1900.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>외국분들 댓글 너무 착하셔세봉이들이 보고 감동했으면 좋겠네요</td>\n",
              "      <td>72.0</td>\n",
              "      <td>(ko)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not a fan but i should say that this song is o...</td>\n",
              "      <td>625.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4 years have passed and this song is still a l...</td>\n",
              "      <td>96.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2319905</th>\n",
              "      <td>or the rapper</td>\n",
              "      <td>0.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2319906</th>\n",
              "      <td>yes yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2319907</th>\n",
              "      <td>the guy in the middle</td>\n",
              "      <td>0.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2319908</th>\n",
              "      <td>victon 00 11 han seungwoo [ leader rapper voca...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2319909</th>\n",
              "      <td>omg thanks fam</td>\n",
              "      <td>1.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2309987 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         0        1     2\n",
              "0        even if you re not a carat let s all admit it ...  28000.0  (en)\n",
              "1        this song tells that some song s don t need bi...   1900.0  (en)\n",
              "2                        외국분들 댓글 너무 착하셔세봉이들이 보고 감동했으면 좋겠네요     72.0  (ko)\n",
              "3        not a fan but i should say that this song is o...    625.0  (en)\n",
              "4        4 years have passed and this song is still a l...     96.0  (en)\n",
              "...                                                    ...      ...   ...\n",
              "2319905                                      or the rapper      0.0  (en)\n",
              "2319906                                            yes yes      0.0  (en)\n",
              "2319907                              the guy in the middle      0.0  (en)\n",
              "2319908  victon 00 11 han seungwoo [ leader rapper voca...      0.0  (en)\n",
              "2319909                                     omg thanks fam      1.0  (en)\n",
              "\n",
              "[2309987 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "q7IOLfuY7kHa",
        "outputId": "9152a1b0-025b-4e7a-f7f7-864cb6110332"
      },
      "source": [
        "df.columns = ['comment','like','lang']\n",
        "# len(df['comment'])\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>like</th>\n",
              "      <th>lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>even if you re not a carat let s all admit it ...</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this song tells that some song s don t need bi...</td>\n",
              "      <td>1900.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>외국분들 댓글 너무 착하셔세봉이들이 보고 감동했으면 좋겠네요</td>\n",
              "      <td>72.0</td>\n",
              "      <td>(ko)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not a fan but i should say that this song is o...</td>\n",
              "      <td>625.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4 years have passed and this song is still a l...</td>\n",
              "      <td>96.0</td>\n",
              "      <td>(en)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment     like  lang\n",
              "0  even if you re not a carat let s all admit it ...  28000.0  (en)\n",
              "1  this song tells that some song s don t need bi...   1900.0  (en)\n",
              "2                  외국분들 댓글 너무 착하셔세봉이들이 보고 감동했으면 좋겠네요     72.0  (ko)\n",
              "3  not a fan but i should say that this song is o...    625.0  (en)\n",
              "4  4 years have passed and this song is still a l...     96.0  (en)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzUfqpol7lR5",
        "outputId": "a1f66660-b201-4d3e-98f0-aa23ab715ec9"
      },
      "source": [
        "data_ko = pd.DataFrame([kor[:1] for kor in df.values if kor[2] == '(ko)'], columns=['comment'])\n",
        "data_en = pd.DataFrame([en[:1] for en in df.values if en[2] == '(en)'], columns=['comment'])\n",
        "data_en.comment\n",
        "len(data_en)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1803300"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fvFSIRYfDTm",
        "outputId": "be653363-3784-4606-b734-53fa8a0914ff"
      },
      "source": [
        "data_en['comment']"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          even if you re not a carat let s all admit it ...\n",
              "1          this song tells that some song s don t need bi...\n",
              "2          not a fan but i should say that this song is o...\n",
              "3          4 years have passed and this song is still a l...\n",
              "4          i m not a fan of them but i really like all of...\n",
              "                                 ...                        \n",
              "1803295                                        or the rapper\n",
              "1803296                                              yes yes\n",
              "1803297                                the guy in the middle\n",
              "1803298    victon 00 11 han seungwoo [ leader rapper voca...\n",
              "1803299                                       omg thanks fam\n",
              "Name: comment, Length: 1803300, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTuEMLr7fKu5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "import re"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4_Nwfsh8Qw-"
      },
      "source": [
        "# 데이터 랜덤으로 추출(10만개 댓글)\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# a = data_en.comment[:50000]\n",
        "# z = random.shuffle(a)\n",
        "# len(z)\n",
        "\n",
        "a = np.random.choice(data_en.comment.values, 100000, replace = False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-62t_zvLyNl",
        "outputId": "459d6126-341d-4eb8-dfc2-0703409b30a4"
      },
      "source": [
        "a"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['welcome uwu if you need any help i d be happy to help',\n",
              "       'i feel so energetic listening to this song gfriend < 3',\n",
              "       'this is a reason to wake up lol', ..., 'in your dream',\n",
              "       'ong yung shin well we didn t say anything about snsd doing plastic surgery we just simply said our opinions on why snsd still dominates the kpop industry and it is due to the fans they ve gained in 9 yrs we did not say anything about one group being more popular due to their looks but as a panda i would still think that my girls are the most beautiful girl group ever lol and the comparison you wrote up their doesn t give your idols any justice by comparing their looks to after school like come on dude this isn t a beauty pageant this is the kpop industry you don t need to be beautiful to have talent or be an amazing singer like eunji when she first started her manager found her from a talent show and she also won first place on the kbs singing show as a child now that s what i mean by talent not those idols who are just discovered randomly in public places and noticed by recruiters due to their looks and i do agree f x is quite versatile but the only one i really like is amber because she has it all < 3 wayy too cool for sm xd and she would ve grown more in yg where most of there are many talented rapper sunbaes that could teach or work with her like tablo teddy masta wu gd top cl and younger members like bi bobby mino etc',\n",
              "       'i miss this sound in kpop'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7NNk2LsIlTU"
      },
      "source": [
        "a_df = pd.DataFrame(a)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "qFJ2pLhgNwDp",
        "outputId": "5b1f130b-0bff-4963-ec28-7470ed6f585d"
      },
      "source": [
        "a_df.columns = ['comment']\n",
        "a_df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>welcome uwu if you need any help i d be happy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i feel so energetic listening to this song gfr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is a reason to wake up lol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24 in belize but its great tho hope more peopl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ii s so sad i lost some thing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>ness can you recommend me which iph colour sho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>ong yung shin well we didn t say anything abou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>in your dream</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>ong yung shin well we didn t say anything abou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>i miss this sound in kpop</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 comment\n",
              "0      welcome uwu if you need any help i d be happy ...\n",
              "1      i feel so energetic listening to this song gfr...\n",
              "2                        this is a reason to wake up lol\n",
              "3      24 in belize but its great tho hope more peopl...\n",
              "4                          ii s so sad i lost some thing\n",
              "...                                                  ...\n",
              "99995  ness can you recommend me which iph colour sho...\n",
              "99996  ong yung shin well we didn t say anything abou...\n",
              "99997                                      in your dream\n",
              "99998  ong yung shin well we didn t say anything abou...\n",
              "99999                          i miss this sound in kpop\n",
              "\n",
              "[100000 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ibqR7F4THo-"
      },
      "source": [
        "# boyz, gurl"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxOA-wRqIRx0",
        "outputId": "8e7744e9-d538-458b-c3fb-f3c060a4c208"
      },
      "source": [
        "# for i in range(len(data_en.comment)):\n",
        "#     data_en.comment[i] = str(data_en.comment[i])\n",
        "\n",
        "for i in range(len(a_df.comment)):\n",
        "    a_df.comment[i] = str(a_df.comment[i])\n",
        "\n",
        "# 숫자제거 / 밑줄 제외한 특수문자 제거\n",
        "p = re.compile(\"[0-7]+\")\n",
        "z = re.compile(\"[8-9]+\")\n",
        "q = re.compile(\"\\W+\")\n",
        "r = re.compile('[^a-zA-Z]+')\n",
        "\n",
        "en = []\n",
        "for i in a:\n",
        "    tokens = re.sub(p,\" \",i)\n",
        "    tokens = re.sub(z,\" \",tokens)\n",
        "    tokens = re.sub(q,\" \",tokens)\n",
        "    tokens = re.sub(r,\" \", tokens)\n",
        "    en.append(tokens)\n",
        "len(en)\n",
        "en[:10]\n",
        "\n",
        "# data_ens =  data_en['comment'].sample(\n",
        "#     # n = '50000',\n",
        "#     frac ='0.3',\n",
        "#     replace = False, #복원/비복원 여부. 동일한 행의 샘플링을 두 번 이상 허용할 것인가\n",
        "#     random_state = 1\n",
        "# )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['welcome uwu if you need any help i d be happy to help',\n",
              " 'i feel so energetic listening to this song gfriend ',\n",
              " 'this is a reason to wake up lol',\n",
              " ' in belize but its great tho hope more people give them recognition',\n",
              " 'ii s so sad i lost some thing',\n",
              " 'jennie the mandu no b views no opinion sorry that itzy is beter than you',\n",
              " 'i found youuu yeah',\n",
              " 'let s make it',\n",
              " 'even saying thank you minute by minute second by second is not enough to show you how grateful and proud we are to know you and your music you ve done soooo much things for us and i know you ll do even greater in the future right now although this isn t enough i still want to say thank you your music made such a great impact to me as a person it contributed a lot to the person i am today seventeen thank you for being home to many people',\n",
              " 'i m so excited for next comeback']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtGGgVIvfMut",
        "outputId": "abe4fb4e-f4fb-4dde-c1ab-9ff40f13a0f4"
      },
      "source": [
        "# 불용어 제거\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "stop_words.update(('gurl, girl, girls, members, song, xd, x, kpop, group, anything, etc'))\n",
        "#(('song','group','songs','youtube','views','time','https','girl','girls','people','yes','lol','video','part','member','members', 'look','way','guys','fans','fan'))\n",
        "\n",
        "# # stopwords_gurl, girl, girls, members, song, xd, x, kpop, group, anything, etc\n",
        "# [('talent', 10422), ('group', 9234), ('comment', 8440), ('preach', 7575), ('gurl', 7573), ('show', 7363), ('anything', 6983), ('kpop', 6954), ('idols', 6882), ('industry', 6858), ('looks', 6854), ('song', 6812), ('lol', 6022), ('reason', 5355), ('wake', 5090), ('fans', 4325), ('girl', 4241), ('girls', 4172), ('members', 4138), ('x', 3830), ('work', 3826), ('xd', 3720), ('rapper', 3692), ('sm', 3674), ('school', 3650), ('place', 3582), ('mino', 3478), ('singer', 3462), ('beauty', 3437), ('etc', 3394)]\n",
        "\n",
        "\n",
        "# for i in range(len(data_en.comment[:10000])):\n",
        "#     data_en.comment[i] = str(data_en.comment[i])\n",
        "\n",
        "res=[]\n",
        "for i in range(len(en)):\n",
        "    word_tokens = word_tokenize(en[i])\n",
        "\n",
        "    result = []\n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            result.append(w) \n",
        "    res.append(result)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7-CML7Y8oCY",
        "outputId": "7da3e328-33dd-4a8f-c4e2-6f2d6f5ff4d1"
      },
      "source": [
        "# print(word_tokens) \n",
        "print(res[:10])\n",
        "print(len(res))\n",
        "\n",
        "en_pos = []\n",
        "for i in range(len(res)):\n",
        "    tokens_pos = nltk.pos_tag(res[i])\n",
        "    en_pos.append(tokens_pos)\n",
        "\n",
        "en_pos[:5]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['welcome', 'uwu', 'need', 'help', 'happy', 'help'], ['feel', 'energetic', 'listening', 'song', 'gfriend'], ['reason', 'wake', 'lol'], ['belize', 'great', 'tho', 'hope', 'people', 'give', 'recognition'], ['ii', 'sad', 'lost', 'thing'], ['jennie', 'mandu', 'views', 'opinion', 'sorry', 'itzy', 'beter'], ['found', 'youuu', 'yeah'], ['let', 'make'], ['even', 'saying', 'thank', 'minute', 'minute', 'second', 'second', 'enough', 'show', 'grateful', 'proud', 'know', 'music', 'done', 'soooo', 'much', 'things', 'us', 'know', 'even', 'greater', 'future', 'right', 'although', 'enough', 'still', 'want', 'say', 'thank', 'music', 'made', 'great', 'impact', 'person', 'contributed', 'lot', 'person', 'today', 'seventeen', 'thank', 'home', 'many', 'people'], ['excited', 'next', 'comeback']]\n",
            "100000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('welcome', 'JJ'),\n",
              "  ('uwu', 'JJ'),\n",
              "  ('need', 'NN'),\n",
              "  ('help', 'NN'),\n",
              "  ('happy', 'VB'),\n",
              "  ('help', 'NN')],\n",
              " [('feel', 'VB'),\n",
              "  ('energetic', 'JJ'),\n",
              "  ('listening', 'NN'),\n",
              "  ('song', 'NN'),\n",
              "  ('gfriend', 'NN')],\n",
              " [('reason', 'NN'), ('wake', 'NN'), ('lol', 'NN')],\n",
              " [('belize', 'VB'),\n",
              "  ('great', 'JJ'),\n",
              "  ('tho', 'JJ'),\n",
              "  ('hope', 'NN'),\n",
              "  ('people', 'NNS'),\n",
              "  ('give', 'VBP'),\n",
              "  ('recognition', 'NN')],\n",
              " [('ii', 'NN'), ('sad', 'NN'), ('lost', 'VBD'), ('thing', 'NN')]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scVo1fYIBBqT",
        "outputId": "2cf6cb76-834a-4281-bbac-f2eab9a548f3"
      },
      "source": [
        "# 명사는 NN을 포함하고 있음을 알 수 있음\n",
        "en_NN=[]\n",
        "for i in range(len(en_pos)):\n",
        "    NN_words = []\n",
        "    for word, pos in en_pos[i]:\n",
        "        if 'NN' in pos:\n",
        "            NN_words.append(word)\n",
        "        elif 'NN' in pos:\n",
        "            NN_words.append(word)\n",
        "    en_NN.extend(NN_words)\n",
        "len(en_NN)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "516719"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bScov1Wi7Bd",
        "outputId": "9735cc26-4524-45be-f354-179f7217d14e"
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "stop_words.update(('gurl, girl, girls, members, song, xd, x, kpop, group, anything, etc'))\n",
        "\n",
        "res22=[]\n",
        "for i in range(len(en_NN)):\n",
        "    word_tokens = word_tokenize(en_NN[i])\n",
        "\n",
        "    result = []\n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            result.append(w) \n",
        "    res22.append(result)\n",
        "len(res22)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "516719"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvhB6d6ngQ5c",
        "outputId": "ba763353-8fb2-49c5-898e-820bdefe4b2b"
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "stop_words.update(('gurl, girl, girls, members, song, xd, x, kpop, group, anything, etc'))\n",
        "\n",
        "res11=[]\n",
        "for w in en_NN: \n",
        "    if w not in stop_words: \n",
        "        res11.append(w) \n",
        "\n",
        "len(res11)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "516719"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybrM9Qwe-9om",
        "outputId": "8e99d910-f51b-4b21-c2fe-60168d9042c5"
      },
      "source": [
        "# 형용사\n",
        "\n",
        "en_ADJ=[]\n",
        "for i in range(len(en_pos)):\n",
        "    JJ_words = []\n",
        "    for word, pos in en_pos[i]:\n",
        "        if 'JJ' in pos:\n",
        "            JJ_words.append(word)\n",
        "        elif 'JJ' in pos:\n",
        "            JJ_words.append(word)\n",
        "    en_ADJ.extend(JJ_words)\n",
        "en_ADJ[:10]\n",
        "#len(en_ADJ) #96230"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['welcome',\n",
              " 'uwu',\n",
              " 'energetic',\n",
              " 'great',\n",
              " 'tho',\n",
              " 'itzy',\n",
              " 'minute',\n",
              " 'minute',\n",
              " 'second',\n",
              " 'second']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTwncuva_AWt",
        "outputId": "25feb6a5-c246-4ab5-f14c-ad4ab830f034"
      },
      "source": [
        "# 동사\n",
        "\n",
        "en_VB=[]\n",
        "for i in range(len(en_pos)):\n",
        "    VB_words = []\n",
        "    for word, pos in en_pos[i]:\n",
        "        if 'VB' in pos:\n",
        "            VB_words.append(word)\n",
        "        elif 'VB' in pos:\n",
        "            VB_words.append(word)\n",
        "    en_VB.extend(VB_words)\n",
        "en_VB[:10]\n",
        "#len(en_VB) #95529"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['happy',\n",
              " 'feel',\n",
              " 'belize',\n",
              " 'give',\n",
              " 'lost',\n",
              " 'sorry',\n",
              " 'found',\n",
              " 'let',\n",
              " 'make',\n",
              " 'saying']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L9Mg5--_JXN"
      },
      "source": [
        "## 중복 삭제 \n",
        "\n",
        "# en_NN_uni= pd.unique(en_NN)\n",
        "# en_ADJ_uni= pd.unique(en_ADJ)\n",
        "# en_VB_uni= pd.unique(en_VB)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo7JWhRFAYW6"
      },
      "source": [
        "# a = pd.concat([en_NN_uni,en_ADJ_uni,en_VB_uni])\n",
        "\n",
        "# en_NN_df = pd.DataFrame(en_NN_uni)\n",
        "# en_ADJ_df = pd.DataFrame(en_ADJ_uni)\n",
        "# en_VB_df = pd.DataFrame(en_VB_uni)\n",
        "\n",
        "# df_ADV = pd.DataFrame(en_ADV)\n",
        "# a['df_ADJ_uni']\n",
        "# pd.DataFrame(en_ADJ_uni)\n",
        "# a['df_VB_uni'] \n",
        "# pd.DataFrame(en_VB_uni)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDzO2ZpYRLrR"
      },
      "source": [
        "# # 문장 빈도\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# c = Counter(a) # input type should be a list of words (or tokens)\n",
        "# k1 = 30\n",
        "# print(c.most_common(k1)) # 빈도수 기준 상위 k개 단어 출력"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD3oc7jR_939",
        "outputId": "486ebd7e-ece6-4824-b9de-fceccd8ac349"
      },
      "source": [
        "# 명사 빈도(10만개)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "c = Counter(en_NN) # input type should be a list of words (or tokens)\n",
        "k1 = 30\n",
        "print(c.most_common(k1)) # 빈도수 기준 상위 k개 단어 출력"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('talent', 10419), ('group', 9394), ('comment', 8371), ('preach', 7615), ('gurl', 7614), ('show', 7389), ('kpop', 7069), ('anything', 7002), ('idols', 6904), ('looks', 6874), ('industry', 6858), ('song', 6816), ('lol', 5964), ('reason', 5279), ('wake', 5033), ('girl', 4337), ('fans', 4302), ('girls', 4199), ('members', 4191), ('work', 3798), ('rapper', 3707), ('xd', 3704), ('sm', 3678), ('school', 3655), ('place', 3602), ('singer', 3498), ('mino', 3458), ('beauty', 3455), ('etc', 3426), ('yrs', 3387)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tfDi1BLRFi5"
      },
      "source": [
        "# stopwords_gurl, girl, girls, members, song, xd, x, kpop, group, anything\n",
        "# [('talent', 10422), ('group', 9234), ('comment', 8440), ('preach', 7575), ('gurl', 7573), ('show', 7363), ('anything', 6983), ('kpop', 6954), ('idols', 6882), ('industry', 6858), ('looks', 6854), ('song', 6812), ('lol', 6022), ('reason', 5355), ('wake', 5090), ('fans', 4325), ('girl', 4241), ('girls', 4172), ('members', 4138), ('x', 3830), ('work', 3826), ('xd', 3720), ('rapper', 3692), ('sm', 3674), ('school', 3650), ('place', 3582), ('mino', 3478), ('singer', 3462), ('beauty', 3437), ('etc', 3394)]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFpjpeTTosAY"
      },
      "source": [
        "# # stopwords\n",
        "\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# stop_words = set(stopwords.words('english')) \n",
        "# stop_words.update(('gurl, girl, girls, members, song, xd, x, kpop, group, anything, etc'))\n",
        "\n",
        "# word_tokens = word_tokenize(example_sent)\n",
        " \n",
        "# filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "# filtered_sentence = []\n",
        " \n",
        "# for w in word_tokens:\n",
        "#     if w not in stop_words:\n",
        "#         filtered_sentence.append(w)\n",
        " \n",
        "# print(word_tokens)\n",
        "# print(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SU5zsXXo53j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad1a734-1313-4403-8440-e61294f866ba"
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "stop_words.update(('gurl, girl, girls, members, song, xd, x, kpop, group, anything, etc'))\n",
        "\n",
        "res22=[]\n",
        "for i in range(len(en_NN)):\n",
        "    word_tokens = word_tokenize(en_NN[i])\n",
        "\n",
        "    result = []\n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            result.append(w) \n",
        "    res22.append(result)\n",
        "len(res22)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "516719"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVDGIieIhUa9"
      },
      "source": [
        "res=[]\n",
        "for i in range(len(en)):\n",
        "    word_tokens = word_tokenize(en[i])\n",
        "\n",
        "    result = []\n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            result.append(w) \n",
        "    res.append(result)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcfLQLRTYT6x",
        "outputId": "723f041a-6b08-4b2d-ed89-b0a53794b152"
      },
      "source": [
        "# 3단어 이하 짧은 단어 제거\n",
        "# remove words less than three letters\n",
        "\n",
        "print(res[1])\n",
        "for word in res[1]:\n",
        "    print(word)\n",
        "\n",
        "en_sent_less3=[]\n",
        "for i in range(len(res)):\n",
        "    tokens = [word for word in res[i] if len(word) >= 3]\n",
        "    en_sent_less3.append(tokens)\n",
        "en_sent_less3[:2]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['feel', 'energetic', 'listening', 'song', 'gfriend']\n",
            "feel\n",
            "energetic\n",
            "listening\n",
            "song\n",
            "gfriend\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['welcome', 'uwu', 'need', 'help', 'happy', 'help'],\n",
              " ['feel', 'energetic', 'listening', 'song', 'gfriend']]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX6JXxThYW9e",
        "outputId": "1d72d2ac-895a-4bae-c663-42b34d85101f"
      },
      "source": [
        "en_sent =[]\n",
        "for i in range(len(en_sent_less3)):\n",
        "    temp=\" \".join(en_sent_less3[i])\n",
        "    en_sent.append(temp)\n",
        "en_sent[:15]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['welcome uwu need help happy help',\n",
              " 'feel energetic listening song gfriend',\n",
              " 'reason wake lol',\n",
              " 'belize great tho hope people give recognition',\n",
              " 'sad lost thing',\n",
              " 'jennie mandu views opinion sorry itzy beter',\n",
              " 'found youuu yeah',\n",
              " 'let make',\n",
              " 'even saying thank minute minute second second enough show grateful proud know music done soooo much things know even greater future right although enough still want say thank music made great impact person contributed lot person today seventeen thank home many people',\n",
              " 'excited next comeback',\n",
              " 'touchin heart since',\n",
              " 'preach gurl best comment',\n",
              " 'think one yeri',\n",
              " 'bsides also bops check top secret hello yummy reality',\n",
              " 'yes']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X397ZyeTBnob"
      },
      "source": [
        "!pip install text-summarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NdtALtr_hM5",
        "outputId": "c643aee2-fb3c-4d5e-9096-b09add68bd37"
      },
      "source": [
        "!pip install textrank"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textrank in /usr/local/lib/python3.7/dist-packages (0.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "WQNQ1Kpv-1V1",
        "outputId": "eb51434c-c2ab-475d-ebee-dcf1d50facc4"
      },
      "source": [
        "import textrank\n",
        "from textrank import KeywordSummarizer"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-d295e1446d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextrank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeywordSummarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'KeywordSummarizer' from 'textrank' (/usr/local/lib/python3.7/dist-packages/textrank.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "lJskjTfDAFXJ",
        "outputId": "e9092a41-faac-4eae-da21-78037508b060"
      },
      "source": [
        "def filter_tokenizer(sent):\n",
        "    words = sent.split()\n",
        "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
        "    return words\n",
        "\n",
        "keyword_extractor = KeywordSummarizer(\n",
        "    tokenize = filter_tokenizer,\n",
        "    window = -1,\n",
        "    verbose = False\n",
        ")\n",
        "\n",
        "keywords = keyword_extractor.summarize(sents, topk=30)\n",
        "for word, rank in keywords:\n",
        "  print(word)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-e2457e2bb5b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m keyword_extractor = KeywordSummarizer(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KeywordSummarizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTKjJJwdA9dX"
      },
      "source": [
        " tokenize = nltk.tokenizer \n",
        "\n",
        "def scan_vocabulary(sents, tokenize, min_count=2):\n",
        "  counter = Counter(w for sent in sents for w in komoran_tokenize(sent, tokenize))\n",
        "  counter = {w:c for w,c in counter.items() if c >= min_count}\n",
        "  idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
        "  vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
        "  return idx_to_vocab, vocab_to_idx\n",
        "\n",
        "# 두 단어간의 유사도 계산\n",
        "# co-occurrence는 문장 안에서 두 단어의 간격이 window인 횟수(2-8 굳)\n",
        "\n",
        "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2):\n",
        "    counter = defaultdict(int)\n",
        "    for s, tokens_i in enumerate(tokens):\n",
        "        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
        "        n = len(vocabs)\n",
        "        for i, v in enumerate(vocabs):\n",
        "            if window <= 0:\n",
        "                b, e = 0, n\n",
        "            else:\n",
        "                b = max(0, i - window)\n",
        "                e = min(i + window, n)\n",
        "            for j in range(b, e):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                counter[(v, vocabs[j])] += 1\n",
        "                counter[(vocabs[j], v)] += 1\n",
        "    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n",
        "    n_vocabs = len(vocab_to_idx)\n",
        "    return dict_to_mat(counter, n_vocabs, n_vocabs)\n",
        "\n",
        "# dict of dict 형식의 그래프를 scipy의 matrix로 변환\n",
        "def dict_to_mat(d, n_rows, n_cols):\n",
        "    rows, cols, data = [], [], []\n",
        "    for (i, j), v in d.items():\n",
        "        rows.append(i)\n",
        "        cols.append(j)\n",
        "        data.append(v)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "# 그래프 그리기\n",
        "def word_graph(sents, tokenize=None, min_count=2, window=2, min_cooccurrence=2):\n",
        "    idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
        "    tokens = [tokenize(sent) for sent in sents]\n",
        "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\n",
        "    return g, idx_to_vocab\n",
        "\n",
        "# 만들어진 그래프에 pagerank 학습하는 함수 만들기\n",
        "def pagerank(x, df=0.85, max_iter=30):\n",
        "    assert 0 < df < 1\n",
        "\n",
        "    # initialize\n",
        "    A = normalize(x, axis=0, norm='l1')\n",
        "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
        "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
        "\n",
        "    # iteration\n",
        "    for _ in range(max_iter):\n",
        "        R = df * (A * R) + bias\n",
        "\n",
        "    return R\n",
        "\n",
        "# 함수 도출\n",
        "def textrank_keyword(sents, tokenize, min_count, window, min_cooccurrence, df=0.85, max_iter=30, topk=30):\n",
        "    g, idx_to_vocab = word_graph(sents, tokenize, min_count, window, min_cooccurrence)\n",
        "    R = pagerank(g, df, max_iter).reshape(-1)\n",
        "    idxs = R.argsort()[-topk:]\n",
        "    keywords = [(idx_to_vocab[idx], R[idx]) for idx in reversed(idxs)]\n",
        "    return keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij58OKQVbhEu"
      },
      "source": [
        "textrank_keyword()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P2hz32-a9CU"
      },
      "source": [
        "# 핵심 문장 추출\n",
        "\n",
        "def sent_graph(sents, tokenize, similarity, min_count=2, min_sim=0.3):\n",
        "    _, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
        "\n",
        "    tokens = [[w for w in tokenize(sent) if w in vocab_to_idx] for sent in sents]\n",
        "    rows, cols, data = [], [], []\n",
        "    n_sents = len(tokens)\n",
        "    for i, tokens_i in enumerate(tokens):\n",
        "        for j, tokens_j in enumerate(tokens):\n",
        "            if i >= j:\n",
        "                continue\n",
        "            sim = similarity(tokens_i, tokens_j)\n",
        "            if sim < min_sim:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(sim)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
        "\n",
        "def textrank_sent_sim(s1, s2):\n",
        "    n1 = len(s1)\n",
        "    n2 = len(s2)\n",
        "    if (n1 <= 1) or (n2 <= 1):\n",
        "        return 0\n",
        "    common = len(set(s1).intersection(set(s2)))\n",
        "    base = math.log(n1) + math.log(n2)\n",
        "    return common / base\n",
        "\n",
        "def cosine_sent_sim(s1, s2):\n",
        "    if (not s1) or (not s2):\n",
        "        return 0\n",
        "\n",
        "    s1 = Counter(s1)\n",
        "    s2 = Counter(s2)\n",
        "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
        "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
        "    prod = 0\n",
        "    for k, v in s1.items():\n",
        "        prod += v * s2.get(k, 0)\n",
        "    return prod / (norm1 * norm2)\n",
        "\n",
        "def textrank_keysentence(sents, tokenize, min_count, similarity, df=0.85, max_iter=30, topk=5):\n",
        "    g = sent_graph(sents, tokenize, min_count, min_sim, similarity)\n",
        "    R = pagerank(g, df, max_iter).reshape(-1)\n",
        "    idxs = R.argsort()[-topk:]\n",
        "    keysents = [(idx, R[idx], sents[idx]) for idx in reversed(idxs)]\n",
        "    return keysents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky7viW7U__ze",
        "outputId": "20bc2bf3-130a-4543-e35a-593cebb96991"
      },
      "source": [
        "# 형용사 빈도\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "c = Counter(en_ADJ) # input type should be a list of words (or tokens)\n",
        "k2 = 20\n",
        "print(c.most_common(k2)) # 빈도수 기준 상위 k개 단어 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('due', 10136), ('beautiful', 7441), ('many', 4454), ('first', 4337), ('popular', 3799), ('top', 3717), ('amazing', 3659), ('give', 3656), ('mean', 3624), ('cool', 3486), ('kpop', 3447), ('public', 3407), ('younger', 3403), ('f', 3397), ('snsd', 3374), ('shin', 3371), ('child', 3363), ('bi', 3362), ('versatile', 3360), ('looks', 3354)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_QGvtnAAEp2",
        "outputId": "d004f8f7-9321-49ee-f954-e53de6f3a354"
      },
      "source": [
        "# 동사 빈도\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "c = Counter(en_VB) # input type should be a list of words (or tokens)\n",
        "k3 = 20\n",
        "print(c.most_common(k3)) # 빈도수 기준 상위 k개 단어 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('say', 7774), ('think', 4934), ('said', 4307), ('talented', 4151), ('come', 4119), ('need', 4062), ('started', 3709), ('found', 3697), ('singing', 3594), ('lol', 3527), ('agree', 3520), ('discovered', 3481), ('wrote', 3472), ('noticed', 3429), ('comparing', 3397), ('gained', 3388), ('grown', 3372), ('dude', 3367), ('teach', 3362), ('snsd', 3360)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xztfWTnp_M6_"
      },
      "source": [
        "# a = pd.concat([en_NN_uni,en_ADJ_uni,en_VB_uni])\n",
        "\n",
        "a['df_NN_uni'] = pd.DataFrame(en_NN_uni)\n",
        "# df_ADV = pd.DataFrame(en_ADV)\n",
        "a['df_ADJ_uni'] = pd.DataFrame(en_ADJ_uni)\n",
        "a['df_VB_uni'] = pd.DataFrame(en_VB_uni)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D51We9_v_SZI",
        "outputId": "dbb29d53-b336-4e2c-e6e7-2dc1de6105fc"
      },
      "source": [
        "a.columns = ['comment','like','lang']\n",
        "# len(df['comment'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0             yeah i can feel your pain i tried to learn it ...\n",
              "1             they looked so powerful standing on individual...\n",
              "2                    i really miss hearing this song from class\n",
              "3                                        phan yes it means very\n",
              "4             it s been already 4 yrs since this song got re...\n",
              "                                    ...                        \n",
              "49998                                  gimbap kidding i m sorry\n",
              "49999                       6 here in malaysia lets do it carat\n",
              "df_NN_uni                   0\n",
              "0          yeah\n",
              "1          fee...\n",
              "df_ADJ_uni                     0\n",
              "0             lack\n",
              "1       ...\n",
              "df_VB_uni                      0\n",
              "0            tried\n",
              "1       ...\n",
              "Name: comment, Length: 50003, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}