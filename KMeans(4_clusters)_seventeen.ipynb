{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KMeans(4_clusters)_seventeen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1b7668YbgrxR992-za21rJQ3OArSIXg8Q",
      "authorship_tag": "ABX9TyOHqb6iKmvcrUJtNXJ1yZwT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IDF13/mulcam_army/blob/sumin/KMeans(4_clusters)_seventeen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIzEXB_CkWsX",
        "outputId": "fb5d84cb-e979-4fe4-f38f-92cc756aac50"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"k-means_code.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Ls1XFeatzdMivoFijtX0MuN9E7ADviAC\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "\"\"\"# 데이터 불러오기\n",
        "댓글을 얼마나 불러와야 할까\n",
        "\"\"\"\n",
        "\n",
        "# 데이터 불러오기\n",
        "\n",
        "path = '/content/drive/MyDrive/[공유] Mulcam_Army 공유폴더!/크롤링 한 자료/k-pop_Radar아티스트 크롤링 할당분/수민_결과물/'\n",
        "comment_file = 'prepro_stats_page_640세븐틴.csv' \n",
        "\n",
        "data = pd.read_csv(path+comment_file, encoding='utf-8', header=None)\n",
        "data.columns = ['comment','like','lang']\n",
        "print(len(data))\n",
        "data.head()\n",
        "\n",
        "data_ko = pd.DataFrame([kor[:1] for kor in data.values if kor[2] == '(ko)'], columns=['comment'])\n",
        "data_en = pd.DataFrame([en[:1] for en in data.values if en[2] == '(en)'], columns=['comment'])\n",
        "data_en.comment.values\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "for i in range(len(data_en.comment)):\n",
        "    data_en.comment[i] = str(data_en.comment[i])\n",
        "\n",
        "# 숫자제거 / 밑줄 제외한 특수문자 제거\n",
        "p = re.compile(\"[0-7]+\")\n",
        "z = re.compile(\"[8-9]+\")\n",
        "q = re.compile(\"\\W+\")\n",
        "r = re.compile('[^a-zA-Z]+')\n",
        "\n",
        "en = []\n",
        "for i in data_en.comment.values:\n",
        "    tokens = re.sub(p,\" \",i)\n",
        "    tokens = re.sub(z,\" \",tokens)\n",
        "    tokens = re.sub(q,\" \",tokens)\n",
        "    tokens = re.sub(r,\" \", tokens)\n",
        "    en.append(tokens)\n",
        "len(en)\n",
        "en[:2]\n",
        "\n",
        "# 불용어 제거\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "# stop_words.update(('song','group','songs','youtube','views','time','https','girl','girls','people','yes','lol','video','part','member','members', 'look','way','guys','fans','fan'))\n",
        "\n",
        "res=[]\n",
        "for i in range(len(en)):\n",
        "    word_tokens = word_tokenize(en[i])\n",
        "\n",
        "    result = []\n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            result.append(w) \n",
        "    res.append(result)\n",
        "\n",
        "# print(word_tokens) \n",
        "print(res[:10])\n",
        "print(len(res))\n",
        "\n",
        "en_pos = []\n",
        "for i in range(len(res)):\n",
        "    tokens_pos = nltk.pos_tag(res[i])\n",
        "    en_pos.append(tokens_pos)\n",
        "\n",
        "en_pos[:5]\n",
        "\n",
        "# 명사는 NN을 포함하고 있음을 알 수 있음\n",
        "en_NN=[]\n",
        "for i in range(len(en_pos)):\n",
        "    NN_words = []\n",
        "    for word, pos in en_pos[i]:\n",
        "        if 'NN' in pos:\n",
        "            NN_words.append(word)\n",
        "        elif 'NN' in pos:\n",
        "            NN_words.append(word)\n",
        "    en_NN.extend(NN_words)\n",
        "en_NN[:10]\n",
        "\n",
        "# df = pd.DataFrame(en_NN)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83238\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[['even', 'carat', 'let', 'admit', 'song', 'universal'], ['song', 'tells', 'song', 'need', 'billion', 'views', 'prove', 'legendary'], ['fan', 'say', 'song', 'another', 'level'], ['years', 'passed', 'song', 'still', 'legend', 'probably', 'legacy', 'passed', 'different', 'generation', 'generation', 'attacca', 'released', 'october', 'hope', 'able', 'reach', 'new', 'milestone', 'boys', 'lately', 'previous', 'comebacks', 'struggle', 'make', 'views', 'go', 'together', 'almost', 'years', 'ripe', 'already', 'still', 'lack', 'effort', 'whilst', 'seventeen', 'work', 'hard', 'satisfy', 'us', 'every', 'menu', 'aim', 'make', 'us', 'taste', 'unending', 'gose', 'segments', 'contents', 'hope', 'able', 'pay', 'back'], ['fan', 'really', 'like', 'songs', 'especially', 'one', 'deserves', 'attention', 'views'], ['funfact', 'everyone', 'agree', 'successful', 'comeback', 'seventeen'], ['proof', 'choreography', 'need', 'jumps', 'flips', 'beautifully', 'done', 'goes', 'well', 'music', 'probably', 'one', 'favorites', 'also', 'video', 'choreography', 'danced', 'x', 'speed', 'left', 'awe', 'well', 'synchronized'], ['forget', 'fandom', 'carat', 'comes', 'song'], ['proves', 'songs', 'need', 'badass', 'become', 'addictive'], ['seventeen', 'stan', 'like', 'guys', 'deserves', 'views', 'likes', 'masterpiece']]\n",
            "69835\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['admit',\n",
              " 'song',\n",
              " 'universal',\n",
              " 'song',\n",
              " 'tells',\n",
              " 'views',\n",
              " 'fan',\n",
              " 'level',\n",
              " 'years',\n",
              " 'song']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsPhE4L_6UjY",
        "outputId": "9b00d6a5-4efc-4331-e01b-704078818db0"
      },
      "source": [
        "## 3단어 이하 짧은 단어 제거\n",
        " # remove words less than three letters\n",
        "# print(res[1])\n",
        "# for word in res[1]:\n",
        "#     print(word)\n",
        "en_sent_less3=[]\n",
        "for i in range(len(res)):\n",
        "    tokens = [word for word in res[i] if len(word) >= 3]\n",
        "    en_sent_less3.append(tokens)\n",
        "en_sent_less3[:2]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['even', 'carat', 'let', 'admit', 'song', 'universal'],\n",
              " ['song', 'tells', 'song', 'need', 'billion', 'views', 'prove', 'legendary']]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQjt3WNR6dFW",
        "outputId": "3f929e7e-7491-4283-d5c1-0c976d9d02fc"
      },
      "source": [
        "en_sent =[]\n",
        "for i in range(len(en_sent_less3)):\n",
        "    temp=\" \".join(en_sent_less3[i])\n",
        "    en_sent.append(temp)\n",
        "en_sent[:15]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['even carat let admit song universal',\n",
              " 'song tells song need billion views prove legendary',\n",
              " 'fan say song another level',\n",
              " 'years passed song still legend probably legacy passed different generation generation attacca released october hope able reach new milestone boys lately previous comebacks struggle make views together almost years ripe already still lack effort whilst seventeen work hard satisfy every menu aim make taste unending gose segments contents hope able pay back',\n",
              " 'fan really like songs especially one deserves attention views',\n",
              " 'funfact everyone agree successful comeback seventeen',\n",
              " 'proof choreography need jumps flips beautifully done goes well music probably one favorites also video choreography danced speed left awe well synchronized',\n",
              " 'forget fandom carat comes song',\n",
              " 'proves songs need badass become addictive',\n",
              " 'seventeen stan like guys deserves views likes masterpiece',\n",
              " 'today wan cry year anniversary thank seventeen give masterpiece song',\n",
              " 'remember watching way back release stanning svt really like hoshi without knowing deeper',\n",
              " 'seventeen song recommendations non fans home lean listen secret highlight change heaven cloud fast pace habit trauma good meplease give discography chance seventeen lot great songs',\n",
              " 'suppose cry title say really stop crying time find masterpiece comforting',\n",
              " 'nobody crys cuz crush love wan cry']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q30F-7Ii6mhg"
      },
      "source": [
        "data_en['en_sent']=en_sent"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "b5R_g5Zn6oMF",
        "outputId": "1a9d96a7-99f5-44ea-9185-922776ec3669"
      },
      "source": [
        "data_en.tail()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>en_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>69830</th>\n",
              "      <td>im updated but i want to binge watch more svt ...</td>\n",
              "      <td>updated want binge watch svt videos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69831</th>\n",
              "      <td>anyways im currently binge watching their vliv...</td>\n",
              "      <td>anyways currently binge watching vlives help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69832</th>\n",
              "      <td>novesuity dino thank you tho 🥰</td>\n",
              "      <td>novesuity dino thank tho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69833</th>\n",
              "      <td>lui quinto did you watch seventeen project big...</td>\n",
              "      <td>lui quinto watch seventeen project big debut a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69834</th>\n",
              "      <td>막내온탑 seventeen im currently watching those</td>\n",
              "      <td>seventeen currently watching</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 comment                                            en_sent\n",
              "69830  im updated but i want to binge watch more svt ...                updated want binge watch svt videos\n",
              "69831  anyways im currently binge watching their vliv...       anyways currently binge watching vlives help\n",
              "69832                     novesuity dino thank you tho 🥰                           novesuity dino thank tho\n",
              "69833  lui quinto did you watch seventeen project big...  lui quinto watch seventeen project big debut a...\n",
              "69834         막내온탑 seventeen im currently watching those                       seventeen currently watching"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "KqeGJs2JktNt",
        "outputId": "e3506f07-f2e4-4014-c2c1-92cde58c2536"
      },
      "source": [
        "# TF_IDF 벡터화\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "docs = data_en\n",
        "# len(docs)\n",
        "docs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>en_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>even if you re not a carat let s all admit it ...</td>\n",
              "      <td>even carat let admit song universal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this song tells that some song s don t need bi...</td>\n",
              "      <td>song tells song need billion views prove legen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not a fan but i should say that this song is o...</td>\n",
              "      <td>fan say song another level</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 years have passed and this song is still a l...</td>\n",
              "      <td>years passed song still legend probably legacy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i m not a fan of them but i really like all of...</td>\n",
              "      <td>fan really like songs especially one deserves ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69830</th>\n",
              "      <td>im updated but i want to binge watch more svt ...</td>\n",
              "      <td>updated want binge watch svt videos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69831</th>\n",
              "      <td>anyways im currently binge watching their vliv...</td>\n",
              "      <td>anyways currently binge watching vlives help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69832</th>\n",
              "      <td>novesuity dino thank you tho 🥰</td>\n",
              "      <td>novesuity dino thank tho</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69833</th>\n",
              "      <td>lui quinto did you watch seventeen project big...</td>\n",
              "      <td>lui quinto watch seventeen project big debut a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69834</th>\n",
              "      <td>막내온탑 seventeen im currently watching those</td>\n",
              "      <td>seventeen currently watching</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>69835 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 comment                                            en_sent\n",
              "0      even if you re not a carat let s all admit it ...                even carat let admit song universal\n",
              "1      this song tells that some song s don t need bi...  song tells song need billion views prove legen...\n",
              "2      not a fan but i should say that this song is o...                         fan say song another level\n",
              "3      4 years have passed and this song is still a l...  years passed song still legend probably legacy...\n",
              "4      i m not a fan of them but i really like all of...  fan really like songs especially one deserves ...\n",
              "...                                                  ...                                                ...\n",
              "69830  im updated but i want to binge watch more svt ...                updated want binge watch svt videos\n",
              "69831  anyways im currently binge watching their vliv...       anyways currently binge watching vlives help\n",
              "69832                     novesuity dino thank you tho 🥰                           novesuity dino thank tho\n",
              "69833  lui quinto did you watch seventeen project big...  lui quinto watch seventeen project big debut a...\n",
              "69834         막내온탑 seventeen im currently watching those                       seventeen currently watching\n",
              "\n",
              "[69835 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9QzRrGv7OOr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "3a886b78-1501-456e-e7f8-1d68c2664ab0"
      },
      "source": [
        "tfidf = TfidfVectorizer(stop_words = 'english', \n",
        "                        #min_df = 3,  # 3회 미만으로 등장하는 토큰은 무시\n",
        "                        max_df = 0.95 # 많이 등장한 단어 5%의 토큰도 무시\n",
        "                        )\n",
        "docs_tf = tfidf.fit_transform(docs.comment.values)\n",
        "\n",
        "# (stop_words='english')\n",
        "# token_pattern='(?u)\\\\b\\\\w+\\\\b' or 't\\w+'\n",
        "# ngram_range : 단어장 생성에 필요한 토큰의 크기       \n",
        "# list일 경우 : fit['']\n",
        "# https://wikidocs.net/33661 -> tf-idf 매개변수"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4f60356f5877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tfidf = TfidfVectorizer(stop_words = 'english', \n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0;31m#min_df = 3,  # 3회 미만으로 등장하는 토큰은 무시\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;31m# 많이 등장한 단어 5%의 토큰도 무시\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         )\n\u001b[1;32m      5\u001b[0m \u001b[0mdocs_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DXO-_yasx84",
        "outputId": "f131ffed-e032-4d52-ebb6-76a93826df2d"
      },
      "source": [
        "docs.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69835, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtXKktlO-O3e",
        "outputId": "2ecfd237-b8b8-4901-8219-d59ca120f0f0"
      },
      "source": [
        "docs_tf.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69835, 47677)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApeAyfIvb6mD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9476f7-f5eb-42fc-ee27-87458ceb6ddf"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "modelkmeans = KMeans(n_clusters =4, init='k-means++', n_init=300)\n",
        "modelkmeans.fit(docs_tf)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=4, n_init=300, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow7CgSynZpgR"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "inertia_arr = []\n",
        "k_range = range(1,11)\n",
        "\n",
        "for k in k_range :\n",
        "\n",
        "  km = KMeans(n_clusters=4, random_state=200)\n",
        "  km.fit(docs_tf)\n",
        "  interia = km.inertia_\n",
        "\n",
        "  print('k :', k, 'interia :', interia)\n",
        "\n",
        "  inertia_arr.append(interia)\n",
        "\n",
        "inertia_arr1 = np.array(inertia_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewc3d27cZuGf"
      },
      "source": [
        "plt.plot(k_range, inertia_arr, marker='o')\n",
        "plt.vlines(3, ymin=inertia_arr.min()*0.9999, ymax=inertia_arr.max()*1.0003, linestyles='--', colors = 'g')\n",
        "plt.vlines(4, ymin=inertia_arr.min()*0.9999, ymax=inertia_arr.max()*1.0003, linestyles='--', colors = 'r')\n",
        "\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()\n",
        "# k=3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s51Op27OLss"
      },
      "source": [
        "# 군집화한 레이블값들을 document_df 에 추가하기\n",
        "\n",
        "document_df['cluster_label'] = cluster_label\n",
        "print(document_df.sort_values(by=['cluster_label']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWtSRk3o9gA8"
      },
      "source": [
        "cluster_centers =  modelkmeans.cluster_centers_"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbMlrAY5OYSr",
        "outputId": "0c4c4978-73bd-4e87-911a-337882abd101"
      },
      "source": [
        "print(cluster_centers.shape)\n",
        "print(cluster_centers)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 47677)\n",
            "[[2.34696254e-03 3.71346633e-05 9.10290789e-06 ... 0.00000000e+00\n",
            "  1.79169727e-05 1.26692129e-05]\n",
            " [2.49781210e-04 0.00000000e+00 0.00000000e+00 ... 5.89872641e-05\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JiwFfFKCF1y",
        "outputId": "8196fc26-fdea-4baa-a2ca-6361f9753830"
      },
      "source": [
        "np.unique(modelkmeans.labels_)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIruZ2WFEjKW"
      },
      "source": [
        "cluster_label = modelkmeans.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLweiMk2E1Li"
      },
      "source": [
        "docs['cluster_label'] =  modelkmeans.labels_"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "FtI5G3qyFA4I",
        "outputId": "4607a7be-0a2b-436b-fc3b-c5d63975b685"
      },
      "source": [
        "docs"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>en_sent</th>\n",
              "      <th>cluster_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>even if you re not a carat let s all admit it ...</td>\n",
              "      <td>even carat let admit song universal</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this song tells that some song s don t need bi...</td>\n",
              "      <td>song tells song need billion views prove legen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not a fan but i should say that this song is o...</td>\n",
              "      <td>fan say song another level</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 years have passed and this song is still a l...</td>\n",
              "      <td>years passed song still legend probably legacy...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i m not a fan of them but i really like all of...</td>\n",
              "      <td>fan really like songs especially one deserves ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69830</th>\n",
              "      <td>im updated but i want to binge watch more svt ...</td>\n",
              "      <td>updated want binge watch svt videos</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69831</th>\n",
              "      <td>anyways im currently binge watching their vliv...</td>\n",
              "      <td>anyways currently binge watching vlives help</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69832</th>\n",
              "      <td>novesuity dino thank you tho 🥰</td>\n",
              "      <td>novesuity dino thank tho</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69833</th>\n",
              "      <td>lui quinto did you watch seventeen project big...</td>\n",
              "      <td>lui quinto watch seventeen project big debut a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69834</th>\n",
              "      <td>막내온탑 seventeen im currently watching those</td>\n",
              "      <td>seventeen currently watching</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>69835 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 comment  ... cluster_label\n",
              "0      even if you re not a carat let s all admit it ...  ...             1\n",
              "1      this song tells that some song s don t need bi...  ...             1\n",
              "2      not a fan but i should say that this song is o...  ...             1\n",
              "3      4 years have passed and this song is still a l...  ...             0\n",
              "4      i m not a fan of them but i really like all of...  ...             0\n",
              "...                                                  ...  ...           ...\n",
              "69830  im updated but i want to binge watch more svt ...  ...             0\n",
              "69831  anyways im currently binge watching their vliv...  ...             0\n",
              "69832                     novesuity dino thank you tho 🥰  ...             3\n",
              "69833  lui quinto did you watch seventeen project big...  ...             0\n",
              "69834         막내온탑 seventeen im currently watching those  ...             1\n",
              "\n",
              "[69835 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9nbZ5LiX2RB"
      },
      "source": [
        "# 클러스터들의 핵심단어 추출\n",
        "# https://techblog-history-younghunjo1.tistory.com/114\n",
        "\n",
        "def get_cluster_details(cluster_model, cluster_data, feature_names,\n",
        "                       cluster_num, top_n_features=10):\n",
        "    cluster_details = {}\n",
        "    # 각 클러스터 레이블별 feature들의 center값들 내림차순으로 정렬 후의 인덱스를 반환\n",
        "    center_feature_idx = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
        "    \n",
        "    # 개별 클러스터 레이블별로 \n",
        "    for cluster_num in range(cluster_num):\n",
        "        # 개별 클러스터별 정보를 담을 empty dict할당\n",
        "        cluster_details[cluster_num] = {}\n",
        "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
        "        \n",
        "        # 각 feature별 center값들 정렬한 인덱스 중 상위 10개만 추출\n",
        "        top_ftr_idx = center_feature_idx[cluster_num, :top_n_features]\n",
        "        top_ftr = [feature_names[idx] for idx in top_ftr_idx]\n",
        "        # top_ftr_idx를 활용해서 상위 10개 feature들의 center값들 반환\n",
        "        # 반환하게 되면 array이기 떄문에 리스트로바꾸기\n",
        "        top_ftr_val = cluster_model.cluster_centers_[cluster_num, top_ftr_idx].tolist()\n",
        "        \n",
        "        # cluster_details 딕셔너리에다가 개별 군집 정보 넣어주기\n",
        "        cluster_details[cluster_num]['top_features'] = top_ftr\n",
        "        cluster_details[cluster_num]['top_featrues_value'] = top_ftr_val\n",
        "\n",
        "        # 해당 cluster_num으로 분류된 파일명(문서들) 넣어주기\n",
        "        doc_cl = cluster_data[cluster_data['cluster_label']==cluster_num]['']\n",
        "\n",
        "        # filenames가 df으로 반환되기 떄문에 값들만 출력해서 array->list로 변환\n",
        "        filenames = doc_cl.values.tolist()\n",
        "        cluster_details[cluster_num][''] = filenames\n",
        "    \n",
        "    return cluster_details"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEh2vklsX7Bb"
      },
      "source": [
        "def print_cluster_details(cluster_details):\n",
        "    for cluster_num, cluster_detail in cluster_details.items():\n",
        "        print(f\"#####Cluster Num: {cluster_num}\")\n",
        "        print()\n",
        "        print(\"상위 10개 feature단어들:\\n\", cluster_detail['top_features'])\n",
        "        print()\n",
        "        print(f\"Cluster {cluster_num}으로 분류된 문서들:\\n{cluster_detail['comment'][:5]}\")\n",
        "        print('-'*20)\n",
        "\n",
        "feature_names = tfidf.get_feature_names()\n",
        "cluster_details = get_cluster_details(cluster_model=km,\n",
        "                                     cluster_data=comment.values,\n",
        "                                     feature_names=feature_names,\n",
        "                                     cluster_num=3,\n",
        "                                     top_n_features=10)\n",
        "print_cluster_details(cluster_details)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9tp_D_lXyJf"
      },
      "source": [
        " # 빈도분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "11SB0FCMQQxb",
        "outputId": "7ccf665c-9ce1-42cc-bbe0-34d4907deec5"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "def scan_vocabulary(sents, tokenize, min_conunt=2): \n",
        "\n",
        "  counter = Counter(w for sent in sents for w in tokenize(sent)) \n",
        "  counter = {w: c for w, c in counter.items() if c >= min_count} \n",
        "  idx_to_vocab = [w for w, _ in sorted(counter.items(), key:lambda x:-x[1])] \n",
        "  vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)} \n",
        "  \n",
        "return idx_to_vocab, vocab_to_idx\n",
        "\n",
        "# 출처: https://ebbnflow.tistory.com/292 [Dev Log : 삶은 확률의 구름]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-6c1b8768c56d>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    idx_to_vocab = [w for w, _ in sorted(counter.items(), key:lambda x:-x[1])]\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biy8kA1dCAVR"
      },
      "source": [
        "# km = KMeans(n_clusters=3)\n",
        "# # inertia_arr = np.array(y).reshape(-1,1)\n",
        "\n",
        "# km.fit(x,y)\n",
        "\n",
        "# check how many unique labels do you have\n",
        "# np.unique(km.labels_)\n",
        "# #array([0, 1, 2], dtype=int32)\n",
        "\n",
        "# km.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FznpHmrfPuzP"
      },
      "source": [
        "# 클러스터링된 문서들 중에서 특정 문서를 하나 선택한 후 비슷한 문서 추출\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "hotel_idx = document_df[document_df['cluster_label']==1].index\n",
        "print(\"호텔 카테고리로 클러스터링된 문서들의 인덱스:\\n\",hotel_idx)\n",
        "print()\n",
        "# 호텔 카테고리로 클러스터링 된 문서들의 인덱스 중 하나 선택해 비교 기준으로 삼을 문서 선정\n",
        "comparison_doc = document_df.iloc[hotel_idx[0]]['filename']\n",
        "print(\"##유사도 비교 기준 문서 이름:\",comparison_doc,'##')\n",
        "print()\n",
        "\n",
        "# 위에서 추출한 호텔 카테고리로 클러스터링된 문서들의 인덱스 중 0번인덱스(비교기준문서)제외한\n",
        "# 다른 문서들과의 유사도 측정\n",
        "similarity = cosine_similarity(ftr_vect[hotel_idx[0]], ftr_vect[hotel_idx])\n",
        "print(similarity)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOxqJbsKP3N8"
      },
      "source": [
        "# 비교기준 문서와 다른 문서들간의 유사도 살펴보기\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# array 내림차순으로 정렬한 후 인덱스 반환 [:,::-1] 모든행에 대해서 열을 내림차순으로!\n",
        "sorted_idx = similarity.argsort()[:,::-1]\n",
        "# 비교문서 당사자는 제외한 인덱스 추출\n",
        "sorted_idx = sorted_idx[:, 1:]\n",
        "\n",
        "# 유사도가 큰 순으로 hotel_idx(label=1인 즉, 호텔과관련된 내용의 문서이름들의 index들)에서 재 정렬 \n",
        "# index로 넣으려면 1차원으로 reshape해주기!\n",
        "hotel_sorted_idx = hotel_idx[sorted_idx.reshape(-1,)]\n",
        "# 유사도 행렬값들을 유사도가 큰 순으로 재정렬(비교 문서 당사자는 제외)\n",
        "hotel_sim_values = np.sort(similarity.reshape(-1,))[::-1]\n",
        "hotel_sim_values = hotel_sim_values[1:]\n",
        "# 이렇게 되면 비교문서와 가장 유사한 순으로 '해당문서의index-유사도값' 으로 동일한 위치가 매핑된 두 개의 array!\n",
        "# 그래서 그대로 데이터프레임의 각 칼럼으로 넣어주기\n",
        "print(hotel_sorted_idx)\n",
        "print(hotel_sim_values)\n",
        "print()\n",
        "print(\"길이 비교\", len(hotel_sorted_idx), len(hotel_sim_values))\n",
        "print()\n",
        "# 빈 데이터프레임 생성\n",
        "hotel_sim_df = pd.DataFrame()\n",
        "# hotel_sorted_idx 와 hotel_sim_values 매핑시킨 array임\n",
        "hotel_sim_df['filename'] = document_df.iloc[hotel_sorted_idx]['filename']\n",
        "hotel_sim_df['similarity'] = hotel_sim_values\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.barplot(data=hotel_sim_df, x='similarity', y='filename')\n",
        "plt.title(comparison_doc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}