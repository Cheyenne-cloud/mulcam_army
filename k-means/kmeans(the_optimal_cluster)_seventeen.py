# -*- coding: utf-8 -*-
"""KMeans(The_Optimal_Cluster)_seventeen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b7668YbgrxR992-za21rJQ3OArSIXg8Q
"""

# -*- coding: utf-8 -*-
"""k-means_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ls1XFeatzdMivoFijtX0MuN9E7ADviAC
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

"""# 데이터 불러오기
댓글을 얼마나 불러와야 할까
"""

# 데이터 불러오기

path = '/content/drive/MyDrive/[공유] Mulcam_Army 공유폴더!/크롤링 한 자료/k-pop_Radar아티스트 크롤링 할당분/수민_결과물/'
comment_file = 'prepro_stats_page_640세븐틴.csv' 

data = pd.read_csv(path+comment_file, encoding='utf-8', header=None)
data.columns = ['comment','like','lang']
print(len(data))
data.head()

data_ko = pd.DataFrame([kor[:1] for kor in data.values if kor[2] == '(ko)'], columns=['comment'])
data_en = pd.DataFrame([en[:1] for en in data.values if en[2] == '(en)'], columns=['comment'])
data_en.comment.values

import nltk
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
import re

for i in range(len(data_en.comment)):
    data_en.comment[i] = str(data_en.comment[i])

# 숫자제거 / 밑줄 제외한 특수문자 제거
p = re.compile("[0-7]+")
z = re.compile("[8-9]+")
q = re.compile("\W+")
r = re.compile('[^a-zA-Z]+')

en = []
for i in data_en.comment.values:
    tokens = re.sub(p," ",i)
    tokens = re.sub(z," ",tokens)
    tokens = re.sub(q," ",tokens)
    tokens = re.sub(r," ", tokens)
    en.append(tokens)
len(en)
en[:2]

# 불용어 제거
import nltk
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

stop_words = set(stopwords.words('english')) 
# stop_words.update(('song','group','songs','youtube','views','time','https','girl','girls','people','yes','lol','video','part','member','members', 'look','way','guys','fans','fan'))

res=[]
for i in range(len(en)):
    word_tokens = word_tokenize(en[i])

    result = []
    for w in word_tokens: 
        if w not in stop_words: 
            result.append(w) 
    res.append(result)

# print(word_tokens) 
print(res[:10])
print(len(res))

en_pos = []
for i in range(len(res)):
    tokens_pos = nltk.pos_tag(res[i])
    en_pos.append(tokens_pos)

en_pos[:5]

# 명사는 NN을 포함하고 있음을 알 수 있음
en_NN=[]
for i in range(len(en_pos)):
    NN_words = []
    for word, pos in en_pos[i]:
        if 'NN' in pos:
            NN_words.append(word)
        elif 'NN' in pos:
            NN_words.append(word)
    en_NN.extend(NN_words)
en_NN[:10]

# df = pd.DataFrame(en_NN)

## 3단어 이하 짧은 단어 제거
 # remove words less than three letters
# print(res[1])
# for word in res[1]:
#     print(word)
en_sent_less3=[]
for i in range(len(res)):
    tokens = [word for word in res[i] if len(word) >= 3]
    en_sent_less3.append(tokens)
en_sent_less3[:2]

en_sent =[]
for i in range(len(en_sent_less3)):
    temp=" ".join(en_sent_less3[i])
    en_sent.append(temp)
en_sent[:15]

data_en['en_sent']=en_sent

data_en.tail()

# TF_IDF 벡터화
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import adjusted_rand_score
from sklearn.cluster import KMeans

docs = data_en
# len(docs)
docs

tfidf = TfidfVectorizer(stop_words=None, 
                        #min_df = 3,  # 3회 미만으로 등장하는 토큰은 무시
                        max_df = 0.95 # 많이 등장한 단어 5%의 토큰도 무시
                        )
docs_tf = tfidf.fit_transform(docs)

# (stop_words='english')
# token_pattern='(?u)\\b\\w+\\b' or 't\w+'
# ngram_range : 단어장 생성에 필요한 토큰의 크기       
# list일 경우 : fit['']
# https://wikidocs.net/33661 -> tf-idf 매개변수

docs.shape

docs_tf.shape

docs_tf[:,0]

docs.tail()

# 데이터 불균형
# df.Class.value_counts(normalize=True).plot(kind='bar')
# print(df.Class.value_counts(normalize=True)*100)

# # oversampling
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler

# scaler = StandardScaler(with_mean=False)
# data_en['scaling'] = scaler.fit_transform(docs_tf)
# # data_en.drop(['overscaling'], axis=1)
# data_en.head()

# docs = data_en
# docs_tf

# data_en['scaling'].shape

type(docs)

type(docs_tf)

docs_tf_df = pd.DataFrame(docs_tf)

type(docs_tf_df)

t = docs.transpose()

docs.shape

t.shape

docs_tf.shape

t.head()

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

x = np.array(t)
y = np.array(docs_tf)

x.shape

docs_tf.shape

y.shape

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=0)
x_train,y_train = smote.fit_sample(x,y)

print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)
print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)
print('SMOTE 적용 후 레이블 값 분포: \n', pd.Series(y_train_over).value_counts())

from sklearn.cluster import KMeans

points = dataset[0]
kmeans = KMeans(n_clusters=4)

kmeans.fit(points)

# plt.scatter(dataset[0][:,0], dataset[0][:,1])
# plt.scatter(dataset[0][:,0], dataset[0][:,1])

x_train, y_train = train_test_split(x, y, test_size=0.3, random_state=0)

print("Number transactions x_train dataset: ", x_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions x_test dataset: ", x_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=0)
x_train,y_train = smote.fit_sample(x,y)

print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)
print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)
print('SMOTE 적용 후 레이블 값 분포: \n', pd.Series(y_train_over).value_counts())

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=0)
X_train_over,y_train_over = smote.fit_sample(X_train,y_train)

print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)
print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)
print('SMOTE 적용 후 레이블 값 분포: \n', pd.Series(y_train_over).value_counts())

tfidf = TfidfVectorizer(stop_words=None, 
                        #min_df = 3,  # 3회 미만으로 등장하는 토큰은 무시
                        max_df = 0.95 # 많이 등장한 단어 5%의 토큰도 무시
                        )
docs_t_tf = tfidf.fit_transform(t)

# As you see the error is AttributeError: 
# 'int' object has no attribute 'lower' which means integer cannot be lower-cased. 
# Somewhere in your code, it tries to lower case integer object which is not possible.
# https://stackoverflow.com/questions/53986123/attributeerror-int-object-has-no-attribute-lower-in-tfidf-and-countvectoriz

# # 1) Convert all rows in your corpus to string object.

# # corpus = ['sentence1', 'sentence 2', 12930, 'sentence 100']
# corpus = [str (item) for item in t]

# # 2) Remove integers in your corpus:

# # corpus = ['sentence1', 'sentence 2', 12930, 'sentence 100']
# corpus = [item for item in corpus if not isinstance(item, int)]

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

x = np.array(t)
y = np.array(docs_tf)

# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)

# print("Number transactions x_train dataset: ", x_train.shape)
# print("Number transactions y_train dataset: ", y_train.shape)
# print("Number transactions x_test dataset: ", x_test.shape)
# print("Number transactions y_test dataset: ", y_test.shape)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=0)
x_train,y_train = smote.fit_sample(x,y)

print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)
print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)
print('SMOTE 적용 후 레이블 값 분포: \n', pd.Series(y_train_over).value_counts())

y.shape

y.dtype

y.size

print([i for i,x in enumerate(list) if len(x) != 560])

# 최적의 군집

inertia_arr = []
k_range = range(1,11)

for k in k_range :

  km = KMeans(n_clusters=k, random_state=200)
  km.fit(docs)
  interia = km.inertia_

  print('k :', k, 'interia :', interia)

  inertia_arr.append(interia)

inertia_arr = np.array(inertia_arr)

#모델링



def modeling(model,x_train,x_test,y_train,y_test):
    model.fit(x_train,y_train)
    pred = model.predict(x_test)
    metrics(y_test,pred)

#평가 지표
def metrics(y_test,pred):
    accuracy = accuracy_score(y_test,pred)
    precision = precision_score(y_test,pred)
    recall = recall_score(y_test,pred)
    f1 = f1_score(y_test,pred)
    roc_score = roc_auc_score(y_test,pred,average='macro')
    print('정확도 : {0:.2f}, 정밀도 : {1:.2f}, 재현율 : {2:.2f}'.format(accuracy,precision,recall))
    print('f1-score : {0:.2f}, auc : {1:.2f}'.format(f1,roc_score,recall))

# X = np.array([[df.tran_cityname, df.tran_signupos, df.tran_signupchannel, df.tran_vmake, df.tran_vmodel, df.tran_vyear]])
# Y = np.array(df['completed_trip_status'].values.tolist())

# smote

# Generate and plot a synthetic imbalanced classification dataset
from collections import Counter
from sklearn.datasets import make_classification
from matplotlib import pyplot
from numpy import where

# define dataset
x, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,
	n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)

# summarize class distribution
counter = Counter(y)
print(counter)
# scatter plot of examples by class label
for label, _ in counter.items():
	row_ix = where(y == label)[0]
	pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))
pyplot.legend()
pyplot.show()

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

x = np.array(data_en.columns !='scaling')
y = np.array(data_en.columns !='scaling')

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)

print("Number transactions x_train dataset: ", x_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions x_test dataset: ", x_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

inertia_arr = []
k_range = range(1,11)

for k in k_range :

  km = KMeans(n_clusters=k, random_state=200)
  km.fit(docs_tf)
  interia = km.inertia_

  print('k :', k, 'interia :', interia)

  inertia_arr.append(interia)

inertia_arr = np.array(inertia_arr)

# docs_tf.reshape()

"""# K-Means clustering"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

# # 최적의 군집 찾기

# distortions = []
# for i in range(1, 11):
#     km = KMeans(
#         n_clusters=i,init='k-means++',
#         n_init=10, max_iter=300,
#         tol=1e-04, random_state=0
#     )
#     km.fit(docs_tf)
#     distortions.append(km.inertia_) # 군집 내 분산, 적을수록 좋음

# # distortions

# centers = km.cluster_centers_
# print(centers)

# # 최적의 군집 시각화(급격하게 줄어드는 부분)

# import matplotlib.pyplot as plt
# import numpy as np

# plt.plot(range(1, 11), distortions, marker='o')
# plt.xlabel('Number of clusters', fontsize=18)
# plt.ylabel('Distortion', fontsize=18)
# plt.show() # k=5

# plt.savefig('/content/drive/MyDrive/[공유] Mulcam_Army 공유폴더!/크롤링 한 자료/k-pop_Radar아티스트 크롤링 할당분/수민_결과물/k-means++_DAY6',  dpi=200, facecolor='#eeeeee', bbox_inches='tight' )

# km.inertia_

# km.score()

inertia_arr = []
k_range = range(1,11)

for k in k_range :

  km = KMeans(n_clusters=k, random_state=200)
  km.fit(docs)
  interia = km.inertia_

  print('k :', k, 'interia :', interia)

  inertia_arr.append(interia)

inertia_arr = np.array(inertia_arr)

plt.plot(k_range, inertia_arr, marker='o')
plt.vlines(3, ymin=inertia_arr.min()*0.9999, ymax=inertia_arr.max()*1.0003, linestyles='--', colors = 'g')
plt.vlines(4, ymin=inertia_arr.min()*0.9999, ymax=inertia_arr.max()*1.0003, linestyles='--', colors = 'r')

plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()
# k=3

from sklearn import datasets
from sklearn.cluster import KMeans
import numpy as np

clusters = km.cluster_centers_

x = clusters.reshape(-1,1)
y = docs_tf.reshape(-1,1)

print(x.shape, y.shape)

km = KMeans(n_clusters=3)
# inertia_arr = np.array(y).reshape(-1,1)

km.fit(x,y)

from sklearn.cluster import KMeans

points = dataset[0]
kmeans = KMeans(n_clusters=4)

kmeans.fit(points)
plt.scatter(dataset[0][:,0], dataset[0][:,1])

plt.scatter(dataset[0][:,0], dataset[0][:,1])

# 클러스터들의 핵심단어 추출

def get_cluster_details(cluster_model, cluster_data, feature_names,
                       cluster_num, top_n_features=10):
    cluster_details = {}
    # 각 클러스터 레이블별 feature들의 center값들 내림차순으로 정렬 후의 인덱스를 반환
    center_feature_idx = cluster_model.cluster_centers_.argsort()[:,::-1]
    
    # 개별 클러스터 레이블별로 
    for cluster_num in range(cluster_num):
        # 개별 클러스터별 정보를 담을 empty dict할당
        cluster_details[cluster_num] = {}
        cluster_details[cluster_num]['cluster'] = cluster_num
        
        # 각 feature별 center값들 정렬한 인덱스 중 상위 10개만 추출
        top_ftr_idx = center_feature_idx[cluster_num, :top_n_features]
        top_ftr = [feature_names[idx] for idx in top_ftr_idx]
        # top_ftr_idx를 활용해서 상위 10개 feature들의 center값들 반환
        # 반환하게 되면 array이기 떄문에 리스트로바꾸기
        top_ftr_val = cluster_model.cluster_centers_[cluster_num, top_ftr_idx].tolist()
        
        # cluster_details 딕셔너리에다가 개별 군집 정보 넣어주기
        cluster_details[cluster_num]['top_features'] = top_ftr
        cluster_details[cluster_num]['top_featrues_value'] = top_ftr_val
        # 해당 cluster_num으로 분류된 파일명(문서들) 넣어주기
        filenames = cluster_data[cluster_data['cluster_label']==cluster_num]['filename']
        # filenames가 df으로 반환되기 떄문에 값들만 출력해서 array->list로 변환
        filenames = filenames.values.tolist()
        cluster_details[cluster_num]['filenames'] = filenames
    
    return cluster_details

def print_cluster_details(cluster_details):
    for cluster_num, cluster_detail in cluster_details.items():
        print(f"#####Cluster Num: {cluster_num}")
        print()
        print("상위 10개 feature단어들:\n", cluster_detail['top_features'])
        print()
        print(f"Cluster {cluster_num}으로 분류된 문서들:\n{cluster_detail['filenames'][:5]}")
        print('-'*20)

feature_names = tfidf.get_feature_names()
cluster_details = get_cluster_details(cluster_model=km,
                                     cluster_data=data_en,
                                     feature_names=feature_names,
                                     cluster_num=3,
                                     top_n_features=10)
print_cluster_details(cluster_details)

# Silhouette Score

from sklearn.metrics import silhouette_score

k_range = range(1,11)

best_n = -1
best_silhouette_score = -1

for k in k_range :
  km = KMeans(n_clusters=k, random_state=200)
  km.fit(docs_tf)
  clusters = km.predict(docs_tf)

  score = silhouette_score(docs_tf, clusters)
  print('k :', k, 'score :', score)

  if score > best_silhouette_score :
    best_n = k
    best_silhouette_score = score

print('best_n : ', best_n, 'best score :', best_silhouette_score)

from sklearn import datasets
from sklearn.cluster import KMeans
import numpy as np

x = clusters.reshape(-1,1)
y = docs_tf.reshape(-1,1)

print(x.shape, y.shape)

km = KMeans(n_clusters=3)
# inertia_arr = np.array(y).reshape(-1,1)

km.fit(x,y)

# check how many unique labels do you have
np.unique(km.labels_)
#array([0, 1, 2], dtype=int32)

km.labels_

# kmeans 최적 군집 시각화 : https://steadiness-193.tistory.com/285

# 디버깅
# ValueError : 레이블 수는 1입니다. silhouette_score를 사용할 때 유효한 값은 2-n_samples-1 (포함)입니다.
# https://pythonq.com/so/python/553343
# https://stackoverflow.com/questions/51382250/valueerror-number-of-labels-is-1-valid-values-are-2-to-n-samples-1-inclusiv

# PCA

from sklearn import decomposition
pca = decomposition.PCA(n_components=3)
sklearn_pca_x = pca.fit_transform(docs_tf)

sklearn_result = pd.DataFrame(sklearn_pca_x, columns=['PC1'])
sklearn_result['y-axis'] = 0.0
sklearn_result['label'] = y

sns.lmplot('PC1', 'y-axis', data=sklearn_result, fit_reg=False,
           scatter_kws={"s" : 50},
           hue='label')

# Silhouette Score

def visualize_silhouette_layer(data, param_init='random', param_n_init=10, param_max_iter=300):
    clusters_range = range(1,11)
    results = []

    for i in clusters_range:
        clusterer = KMeans(n_clusters=i, init=param_init, n_init=param_n_init, max_iter=param_max_iter, random_state=0)
        cluster_labels = clusterer.fit_predict(data)
        silhouette_avg = silhouette_score(data, cluster_labels)
        results.append([i, silhouette_avg])

    result = pd.DataFrame(results, columns=["n_clusters", "silhouette_score"])
    pivot_km = pd.pivot_table(result, index="n_clusters", values="silhouette_score")

    plt.figure()
    sns.heatmap(pivot_km, annot=True, linewidths=.5, fmt='.3f', cmap=sns.cm._rocket_lut)
    plt.tight_layout()
    plt.show()

distortions.reshape(-1,1)

visualize_silhouette_layer(distortions)

def visualize_elbowmethod(data, param_init='random', param_n_init=10, param_max_iter=300):
    distortions = []
    for i in range(1, 10):
        km = KMeans(n_clusters=i, init=param_init, n_init=param_n_init, max_iter=param_max_iter, random_state=0)
        km.fit(data)
        distortions.append(km.inertia_)

    plt.plot(range(1, 10), distortions, marker='o')
    plt.xlabel('Number of Cluster')
    plt.ylabel('Distortion')
    plt.show()